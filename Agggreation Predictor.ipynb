{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "310ce4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from utils.utils import load_data_set, gen_batch_sequence\n",
    "from model.wordEmbedding import WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d535aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset\n",
      "Loaded 56355 queries and 18585 tables\n"
     ]
    }
   ],
   "source": [
    "train_query, train_table = load_data_set('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c1f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_query_id = [34,56,12,43] #random\n",
    "ret_tup = gen_batch_sequence(train_query,train_table,selected_query_id, 0, len(selected_query_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404ced24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['during', 'which', 'years', 'was', 'marcus', 'banks', 'in', 'toronto', '?'], ['what', 'is', 'the', 'canton', 'of', 'grande', 'dixence', '?'], ['what', 'school', 'did', 'player', 'number', '6', 'come', 'from', '?'], ['what', 'time', 'was', 'the', 'highest', 'for', '2nd', 'finishers', '?']]\n"
     ]
    }
   ],
   "source": [
    "batch_query = ret_tup[0]\n",
    "batch_table = ret_tup[1]\n",
    "gt_quety = ret_tup[4]\n",
    "print(batch_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73519a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "word_emb = WordEmbedding('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fba1c1",
   "metadata": {},
   "source": [
    "## Function for running LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6909140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 47, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_op, input_lens = word_emb.gen_x_batch(batch_query, batch_table)\n",
    "x_emb = bert_op.last_hidden_state\n",
    "x_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2061717a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_op.pooler_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e417519",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d289c417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(768, 50, num_layers=4, bidirectional=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(768,hidden_size = 100//2, num_layers = 4,bidirectional=True)\n",
    "\n",
    "lstm.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475e790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_n, (c_fwd, c_rev) = lstm(x_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ccf8830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_n: torch.Size([4, 47, 100])\n",
      "c_wd: torch.Size([8, 47, 50])\n",
      "c_rev: torch.Size([8, 47, 50])\n"
     ]
    }
   ],
   "source": [
    "print(f\"h_n: {h_n.shape}\\nc_wd: {c_fwd.shape}\\nc_rev: {c_rev.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6931dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  in the code for the reference paper they sorted the inputs (acc. to the size)\n",
    "# TODO: Clean this function: \n",
    "def run_lstm(lstm, inp, inp_length, prev_hidden=None):\n",
    "    '''\n",
    "    Input: This function takes in 3 arguments \n",
    "        lstm : the name of the lstm variable that needs to be run\n",
    "        inp  : the input in the for [Batch size , num_tok, last_layer]\n",
    "        inp_length: an array that contains the length of each element in the batch size = batch size\n",
    "        pre_hidden: hidden layer values of the previous lstm layer\n",
    "    \n",
    "    Ouptut: \n",
    "        Same as nn.LSTM\n",
    "    '''\n",
    "    ret_h, ret_c = lstm(inp,prev_hidden)\n",
    "    return ret_h, ret_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49713437",
   "metadata": {},
   "source": [
    "## Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52409987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4,5,6]])\n",
    "x = x.squeeze()\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "426da097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.tensor([[[1,2,3,4],\n",
    "                  [5,6,7,8]],\n",
    "                [[9,10,11,12],\n",
    "                 [13,14,15,16]],\n",
    "                [[9,10,11,12],\n",
    "                 [13,14,15,16]]\n",
    "                ]).to('cuda')\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "363cf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237597f",
   "metadata": {},
   "source": [
    "### Scalar attention score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54216a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_attention = nn.Linear(num_hidden, 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e1f9d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 47])\n"
     ]
    }
   ],
   "source": [
    "att_val = scalar_attention(h_n)\n",
    "att_val = att_val.squeeze()\n",
    "print(att_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13f3b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x_len = max(input_lens)\n",
    "for idx, num in enumerate(input_lens): # reduce the importance of 0 values\n",
    "    if num < max_x_len:\n",
    "        att_val[idx,num:] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bab90e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 47])\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1) # Probability distribution for the attention values\n",
    "att = softmax(att_val)\n",
    "print(att.shape)\n",
    "\n",
    "for x in att:\n",
    "    assert int(x.sum().ceil().tolist()) == 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bfd01bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 47, 100])\n",
      "torch.Size([4, 47, 100])\n",
      "torch.Size([4, 100])\n"
     ]
    }
   ],
   "source": [
    "print(h_n.shape)\n",
    "att_matrix = att.unsqueeze(2).expand_as(h_n)\n",
    "print(att_matrix.shape)\n",
    "K_agg = (h_n*att_matrix).sum(1)\n",
    "print(K_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7c7cd356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 100])\n"
     ]
    }
   ],
   "source": [
    "print(att_matrix[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd7cda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_out = nn.Sequential(\n",
    "            nn.Linear(num_hidden,num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, 6)\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "36b12a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0695,  0.1001,  0.1088,  0.0280,  0.0561, -0.0524],\n",
      "        [-0.0667,  0.0973,  0.1071,  0.0263,  0.0583, -0.0472],\n",
      "        [-0.0653,  0.0935,  0.1049,  0.0239,  0.0583, -0.0464],\n",
      "        [-0.0636,  0.0896,  0.1036,  0.0177,  0.0548, -0.0475]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "agg_score = agg_out(K_agg)\n",
    "print(agg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c20c3",
   "metadata": {},
   "source": [
    "## Class for aggregation predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8eae0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregationPredictor(nn.Module):\n",
    "    def __init__(self,input_layer,hidden_size,num_layers,gpu):\n",
    "        \n",
    "        self.agg_lstm = nn.LSTM(input_size=input_layer, hidden_size=hidden_size // 2,\n",
    "                                num_layers=num_layers, batch_first=True,\n",
    "                                dropout=0.3, bidirectional=True)\n",
    "        \n",
    "        self.agg_att = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.soft_max = nn.softmax()\n",
    "        self.agg_out = nn.Sequential(\n",
    "            nn.Linear(input_size = hidden_size, out_features=hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_size , hidden_size, 6)\n",
    "        )\n",
    "        \n",
    "        if(gpu):\n",
    "            self.agg_lstm = self.agg_lstm.to('cuda')\n",
    "            self.agg_att  = self.agg_att.to('cuda')\n",
    "            self.soft_max = self.soft_max.to('cuda')\n",
    "            self.agg_out  = self.agg_out.to('cuda')\n",
    "    \n",
    "    def forward(x_input, x_len):\n",
    "        B = x_input.shape[0]\n",
    "        max_len = max(x_len)\n",
    "        h_n, _ = run_lstm(agg.lstm,agg.lstm.shape[1]) # [B * longestinput * hidden_size] \n",
    "        #calculate the scalar attention score. [scalar, since one value for each input word.]\n",
    "        att_val = self.agg_att(h_n)  #[B * longest_input * 1]\n",
    "        att_val = att_val.squeeze()  #[B* longest_input]\n",
    "        \n",
    "        for index, l in x_len:\n",
    "            if(l<max_len):\n",
    "                att_val = att_val[index][l:] = -100\n",
    "        \n",
    "        att_prob_dist = self.soft_max(att_val) #[B * longest_input]\n",
    "        att_prob_dist = att_prob_dist.unsqueeze(2).expand_as(h_n) #[B * longest_input * hidden_size]\n",
    "        K_agg = (h_n* att_prob_dist).sum(1)\n",
    "        agg_score = self.agg_out(K_agg)\n",
    "        return agg_score\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SQL(nn.Module):\n",
    "    def __init__(self, bert_model_name, hidden_size, num_depth ):\n",
    "        super(Seq2SQL,self).__init__()\n",
    "        \n",
    "        self.gpu = torch.cuda.is_available()\n",
    "        if(hidden_size&1!=0):\n",
    "            raise ValueError('hidden size must be even, since this is a bidirectional network')\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.aggregator = AggregationPredictor()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
